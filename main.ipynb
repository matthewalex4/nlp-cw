{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e77a1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (2.10.0)\n",
      "Requirement already satisfied: filelock in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (3.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (82.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch) (1.3.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers[torch] in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (5.2.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from transformers[torch]) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from transformers[torch]) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from transformers[torch]) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from transformers[torch]) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from transformers[torch]) (2026.2.19)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from transformers[torch]) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from transformers[torch]) (0.24.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from transformers[torch]) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from transformers[torch]) (4.67.3)\n",
      "Requirement already satisfied: torch>=2.4 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from transformers[torch]) (2.10.0)\n",
      "Requirement already satisfied: accelerate>=1.1.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from transformers[torch]) (1.12.0)\n",
      "Requirement already satisfied: psutil in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from accelerate>=1.1.0->transformers[torch]) (7.2.2)\n",
      "Requirement already satisfied: filelock in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers[torch]) (3.24.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers[torch]) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers[torch]) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers[torch]) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers[torch]) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers[torch]) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (82.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.4->transformers[torch]) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch>=2.4->transformers[torch]) (1.3.4)\n",
      "Requirement already satisfied: typer>=0.24.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from typer-slim->transformers[torch]) (0.24.0)\n",
      "Requirement already satisfied: anyio in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers[torch]) (4.12.1)\n",
      "Requirement already satisfied: certifi in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers[torch]) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers[torch]) (1.0.9)\n",
      "Requirement already satisfied: idna in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers[torch]) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers[torch]) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.4->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: click>=8.2.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from typer>=0.24.0->typer-slim->transformers[torch]) (8.3.1)\n",
      "Requirement already satisfied: rich>=12.3.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from typer>=0.24.0->typer-slim->transformers[torch]) (14.3.3)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from typer>=0.24.0->typer-slim->transformers[torch]) (0.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from jinja2->torch>=2.4->transformers[torch]) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers[torch]) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers[torch]) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers[torch]) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipywidgets in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (8.1.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipywidgets) (9.10.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: decorator>=4.3.2 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.6)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (4.5.0)\n",
      "Requirement already satisfied: filelock in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from datasets) (3.24.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from datasets) (2.4.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from datasets) (23.0.1)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from datasets) (3.0.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from datasets) (4.67.3)\n",
      "Requirement already satisfied: xxhash in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from datasets) (1.4.1)\n",
      "Requirement already satisfied: packaging in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from datasets) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: typer>=0.24.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (0.24.0)\n",
      "Requirement already satisfied: click>=8.2.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
      "Requirement already satisfied: rich>=12.3.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (14.3.3)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.24.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from scikit-learn) (2.4.2)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipywidgets in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (8.1.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipywidgets) (9.10.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: decorator>=4.3.2 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.6)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: accelerate>=1.1.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from accelerate>=1.1.0) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from accelerate>=1.1.0) (26.0)\n",
      "Requirement already satisfied: psutil in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from accelerate>=1.1.0) (7.2.2)\n",
      "Requirement already satisfied: pyyaml in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from accelerate>=1.1.0) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from accelerate>=1.1.0) (2.10.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from accelerate>=1.1.0) (1.4.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from accelerate>=1.1.0) (0.7.0)\n",
      "Requirement already satisfied: filelock in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.1.0) (3.24.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.1.0) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.1.0) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.1.0) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.1.0) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.1.0) (4.67.3)\n",
      "Requirement already satisfied: typer-slim in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.1.0) (0.24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.1.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (82.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (3.1.6)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.1.0) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch>=2.0.0->accelerate>=1.1.0) (1.3.4)\n",
      "Requirement already satisfied: anyio in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=1.1.0) (4.12.1)\n",
      "Requirement already satisfied: certifi in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=1.1.0) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=1.1.0) (1.0.9)\n",
      "Requirement already satisfied: idna in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=1.1.0) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=1.1.0) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=1.1.0) (3.0.3)\n",
      "Requirement already satisfied: typer>=0.24.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from typer-slim->huggingface_hub>=0.21.0->accelerate>=1.1.0) (0.24.0)\n",
      "Requirement already satisfied: click>=8.2.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from typer>=0.24.0->typer-slim->huggingface_hub>=0.21.0->accelerate>=1.1.0) (8.3.1)\n",
      "Requirement already satisfied: rich>=12.3.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from typer>=0.24.0->typer-slim->huggingface_hub>=0.21.0->accelerate>=1.1.0) (14.3.3)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from typer>=0.24.0->typer-slim->huggingface_hub>=0.21.0->accelerate>=1.1.0) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface_hub>=0.21.0->accelerate>=1.1.0) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface_hub>=0.21.0->accelerate>=1.1.0) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /vol/bitbucket/ma4723/myenv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface_hub>=0.21.0->accelerate>=1.1.0) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install transformers[torch]\n",
    "%pip install ipywidgets\n",
    "%pip install datasets\n",
    "%pip install -U scikit-learn\n",
    "%pip install -U ipywidgets\n",
    "%pip install 'accelerate>=1.1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c39609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "pd.set_option('display.width',1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21dda637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "EPOCHS = 3\n",
    "MAX_LENGTH = 128\n",
    "K = 5\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b9f5530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  par_id                                               text  labels\n",
      "0   4341  The scheme saw an estimated 150,000 children f...       1\n",
      "1   4136  Durban 's homeless communities reconciliation ...       1\n",
      "2  10352  The next immediate problem that cropped up was...       1\n",
      "3   8279  Far more important than the implications for t...       1\n",
      "4   1164  To strengthen child-sensitive social protectio...       1\n",
      "\n",
      "  par_id                                               text  labels\n",
      "0   4046  We also know that they can benefit by receivin...       1\n",
      "1   1279  Pope Francis washed and kissed the feet of Mus...       1\n",
      "2   8330  Many refugees do n't want to be resettled anyw...       1\n",
      "3   4063  \"Budding chefs , like \"\" Fred \"\" , \"\" Winston ...       1\n",
      "4   4089  \"In a 90-degree view of his constituency , one...       1\n",
      "\n",
      "Dataset({\n",
      "    features: ['par_id', 'text', 'labels'],\n",
      "    num_rows: 8375\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from load_data import DontPatronizeMe\n",
    "\n",
    "DATA_DIR = 'data/'\n",
    "TEST_PATH = f'{DATA_DIR}task4_test.tsv'\n",
    "\n",
    "dpm = DontPatronizeMe(DATA_DIR, TEST_PATH)\n",
    "dpm.load_task1()\n",
    "\n",
    "data = dpm.train_task1_df\n",
    "\n",
    "trids = pd.read_csv(f'{DATA_DIR}train_semeval_parids-labels.csv')\n",
    "valids = pd.read_csv(f'{DATA_DIR}dev_semeval_parids-labels.csv')\n",
    "\n",
    "trids['par_id'] = trids.par_id.astype(str)\n",
    "valids['par_id'] = valids.par_id.astype(str)\n",
    "\n",
    "cols = ['par_id', 'text', 'label_y']\n",
    "\n",
    "trdf = trids.merge(data, on='par_id', how='left')[cols]\n",
    "valdf = valids.merge(data, on='par_id', how='left')[cols]\n",
    "\n",
    "# rename label_y to label for Trainer\n",
    "trdf.rename(columns={\"label_y\": \"labels\"}, inplace=True)\n",
    "valdf.rename(columns={\"label_y\": \"labels\"}, inplace=True)\n",
    "\n",
    "\n",
    "print(trdf.head())\n",
    "print()\n",
    "\n",
    "print(valdf.head())\n",
    "print()\n",
    "\n",
    "trds = Dataset.from_pandas(trdf)\n",
    "print(trds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58285fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "labels\n",
      "0    7581\n",
      "1     794\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class weights: tensor([1.0000, 9.5479], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts = trdf['labels'].value_counts()\n",
    "print(\"Original class distribution:\")\n",
    "print(counts)\n",
    "print()\n",
    "\n",
    "weights = torch.tensor([counts[0]/counts[0], counts[0]/counts[1]], dtype=torch.float).to(device)\n",
    "print(\"Class weights:\", weights)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc2ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "X = trdf[\"text\"].values\n",
    "y = trdf[\"labels\"].values\n",
    "\n",
    "folds = list(skf.split(X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7b52708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class BalancedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\").view(-1)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.view(-1, 2)\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    f1 = f1_score(labels, preds, pos_label=1)\n",
    "    return { \"f1\": f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea8b4d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "rob_checkpoint = \"FacebookAI/roberta-base\"\n",
    "deb_checkpoint = \"microsoft/deberta-base\"\n",
    "\n",
    "rob_tokenizer = AutoTokenizer.from_pretrained(rob_checkpoint)\n",
    "deb_tokenizer = AutoTokenizer.from_pretrained(deb_checkpoint)\n",
    "\n",
    "def tokenize(tokenizer, batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "architectures = [(\"Roberta\", rob_checkpoint, rob_tokenizer), (\"Deberta\", deb_checkpoint, deb_tokenizer)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4682128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training epochs: 120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LRS = [2e-5, 3e-5]\n",
    "BSZS = [16, 32]\n",
    "\n",
    "num_epochs = len(folds) * len(architectures) * len(LRS) * len(BSZS) * EPOCHS\n",
    "print(f\"Total training epochs: {num_epochs}\")\n",
    "print()\n",
    "\n",
    "# Determine best hyperparameters using k-fold cross-validation\n",
    "def hyperparameter_search():\n",
    "    hp_metrics = {}\n",
    "    hp_metrics[\"Roberta\"] = defaultdict(float)\n",
    "    hp_metrics[\"Deberta\"] = defaultdict(float)\n",
    "\n",
    "    for name, checkpoint, tokenizer in architectures:\n",
    "        print(f\"Training {name}...\")\n",
    "        print()\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "            print(f\"Fold {fold + 1}/{K}\")\n",
    "\n",
    "            trds = Dataset.from_pandas(trdf.iloc[train_idx])\n",
    "            valds = Dataset.from_pandas(trdf.iloc[val_idx])\n",
    "\n",
    "            trds = trds.map(lambda batch: tokenize(tokenizer, batch), batched=True)\n",
    "            valds = valds.map(lambda batch: tokenize(tokenizer, batch), batched=True)\n",
    "\n",
    "            trds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "            valds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "            for lr, bsz in product(LRS, BSZS):\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2).to(device)\n",
    "                training_args = TrainingArguments(\n",
    "                    learning_rate=lr,\n",
    "                    per_device_train_batch_size=bsz,\n",
    "                    per_device_eval_batch_size=bsz,\n",
    "                    num_train_epochs=EPOCHS,\n",
    "                    logging_strategy=\"epoch\",\n",
    "                    eval_strategy=\"epoch\",\n",
    "                    save_strategy=\"no\",\n",
    "                    # save_strategy=\"epoch\",\n",
    "                    # save_total_limit=1,\n",
    "                    # load_best_model_at_end=True,\n",
    "                    # metric_for_best_model=\"f1\",\n",
    "                    remove_unused_columns=False,\n",
    "                )\n",
    "\n",
    "                trainer = BalancedTrainer(\n",
    "                    model=model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=trds,\n",
    "                    eval_dataset=valds,\n",
    "                    compute_metrics=compute_metrics\n",
    "                )\n",
    "\n",
    "                trainer.train()\n",
    "                metrics = trainer.evaluate()\n",
    "\n",
    "                hp_metrics[name][(lr, bsz)] += metrics[\"eval_f1\"]\n",
    "            \n",
    "    res = {}\n",
    "    for name in hp_metrics:\n",
    "        best_hp = max(hp_metrics[name], key=hp_metrics[name].get)\n",
    "        res[name] = {\n",
    "            \"best_hps\": best_hp,\n",
    "            \"avg_f1\": hp_metrics[name][best_hp] / K\n",
    "        }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adcf8035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Roberta...\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec99d4a2acc42c0add64e49f24f802b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7308de8270db4afab621c7dacc1b6172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1675 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4642e6e3a07b43fb9b4327f0bf66dc57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.653023</td>\n",
       "      <td>0.684731</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.536443</td>\n",
       "      <td>0.265858</td>\n",
       "      <td>0.454212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.521442</td>\n",
       "      <td>0.296016</td>\n",
       "      <td>0.501650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6571e0fe239417f9bbe1ed8d5af922c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 03:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.561457</td>\n",
       "      <td>0.256276</td>\n",
       "      <td>0.465649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.358944</td>\n",
       "      <td>0.224560</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.261012</td>\n",
       "      <td>0.272728</td>\n",
       "      <td>0.554090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafc9886419a48cfa53cf912ecf2b611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.650717</td>\n",
       "      <td>0.578916</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.552264</td>\n",
       "      <td>0.305134</td>\n",
       "      <td>0.459677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.445533</td>\n",
       "      <td>0.280158</td>\n",
       "      <td>0.547231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547ef3e54c4e4e6791017dfcde249757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 02:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.576968</td>\n",
       "      <td>0.251539</td>\n",
       "      <td>0.398340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.401458</td>\n",
       "      <td>0.241181</td>\n",
       "      <td>0.486111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.301805</td>\n",
       "      <td>0.282075</td>\n",
       "      <td>0.561934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef7dfe94e4d47759da4196732423d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df60fa3d59af4db2b2cf06d43ba3e817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1675 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a334c586fc264dbfa7338162ab28d458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.585191</td>\n",
       "      <td>0.221789</td>\n",
       "      <td>0.534591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.442349</td>\n",
       "      <td>0.239910</td>\n",
       "      <td>0.502024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.338540</td>\n",
       "      <td>0.297082</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1cfbf8b0de483f9dbcf45467642b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 02:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.526299</td>\n",
       "      <td>0.245521</td>\n",
       "      <td>0.549865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.370247</td>\n",
       "      <td>0.253792</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.233162</td>\n",
       "      <td>0.290084</td>\n",
       "      <td>0.560209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b28c77d19b486aad96cf6f15ba5066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.602557</td>\n",
       "      <td>0.237626</td>\n",
       "      <td>0.380531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.441589</td>\n",
       "      <td>0.255082</td>\n",
       "      <td>0.456897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.357750</td>\n",
       "      <td>0.319995</td>\n",
       "      <td>0.583893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed988162266b44d9843512817cfaef3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 02:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.549193</td>\n",
       "      <td>0.228150</td>\n",
       "      <td>0.549254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.364181</td>\n",
       "      <td>0.229441</td>\n",
       "      <td>0.540059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.214177</td>\n",
       "      <td>0.291018</td>\n",
       "      <td>0.543860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c74f54ccbc426abe261c5d9da13e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1707d2afaff5433e98d963d71e663034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1675 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a8469b034f4a97b3e286cdcd5821cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.608562</td>\n",
       "      <td>0.301465</td>\n",
       "      <td>0.501053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.453702</td>\n",
       "      <td>0.234336</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.331890</td>\n",
       "      <td>0.316216</td>\n",
       "      <td>0.568862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bc41dd8940417b9709a17700128678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 02:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.565889</td>\n",
       "      <td>0.396072</td>\n",
       "      <td>0.535135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.362182</td>\n",
       "      <td>0.202648</td>\n",
       "      <td>0.577778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.248378</td>\n",
       "      <td>0.260279</td>\n",
       "      <td>0.582210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925a435360c44a94a2242aef8c3ce8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.625365</td>\n",
       "      <td>0.401285</td>\n",
       "      <td>0.469298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.451189</td>\n",
       "      <td>0.235495</td>\n",
       "      <td>0.573529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.332042</td>\n",
       "      <td>0.331026</td>\n",
       "      <td>0.575851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec301da9f7244589527e65c307498b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 02:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.567340</td>\n",
       "      <td>0.405836</td>\n",
       "      <td>0.535519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.408803</td>\n",
       "      <td>0.227893</td>\n",
       "      <td>0.574648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.262383</td>\n",
       "      <td>0.299410</td>\n",
       "      <td>0.582633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bc098c8e834b409880b30626571070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd979ed35f584de498eee160aa231fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1675 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cf7b4e067540609d9af11bd4178da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.595188</td>\n",
       "      <td>0.247671</td>\n",
       "      <td>0.492857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.444413</td>\n",
       "      <td>0.275868</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.323813</td>\n",
       "      <td>0.359225</td>\n",
       "      <td>0.506329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d2010d69e649f6af0163e720c81ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 02:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.550034</td>\n",
       "      <td>0.319244</td>\n",
       "      <td>0.510345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.374634</td>\n",
       "      <td>0.289540</td>\n",
       "      <td>0.518703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.240601</td>\n",
       "      <td>0.304312</td>\n",
       "      <td>0.518717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ca61c0d8934ca1a63b6b75382ed486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.618028</td>\n",
       "      <td>0.275064</td>\n",
       "      <td>0.510029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.465135</td>\n",
       "      <td>0.273531</td>\n",
       "      <td>0.510791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.326335</td>\n",
       "      <td>0.355554</td>\n",
       "      <td>0.528662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a607d3c1cdb429aa7a40e75d4c64c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 02:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.574846</td>\n",
       "      <td>0.287093</td>\n",
       "      <td>0.482540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.414097</td>\n",
       "      <td>0.300937</td>\n",
       "      <td>0.548148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.253172</td>\n",
       "      <td>0.301918</td>\n",
       "      <td>0.548913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ccd4699f1e4d92ac57b8d2923aed12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680fbf8302f44b1ba7bd7cff08732379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1675 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481b531a2c274e1cb602b8a35535e9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.601328</td>\n",
       "      <td>0.226340</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.438930</td>\n",
       "      <td>0.256000</td>\n",
       "      <td>0.504202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.313651</td>\n",
       "      <td>0.327651</td>\n",
       "      <td>0.542056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70408023d4f444b839edc0d4e6cea0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 02:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.546106</td>\n",
       "      <td>0.225451</td>\n",
       "      <td>0.515050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.390929</td>\n",
       "      <td>0.238764</td>\n",
       "      <td>0.521246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.265216</td>\n",
       "      <td>0.297513</td>\n",
       "      <td>0.555000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bff57bae94d4033b7ef76921da368a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.618155</td>\n",
       "      <td>0.246743</td>\n",
       "      <td>0.457516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.472141</td>\n",
       "      <td>0.257355</td>\n",
       "      <td>0.503704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.345035</td>\n",
       "      <td>0.357207</td>\n",
       "      <td>0.540373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b294456034a04a23b311b8fcd8eeddd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 02:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.558159</td>\n",
       "      <td>0.241899</td>\n",
       "      <td>0.461017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.394859</td>\n",
       "      <td>0.217909</td>\n",
       "      <td>0.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.274334</td>\n",
       "      <td>0.289332</td>\n",
       "      <td>0.541436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Deberta...\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76cd3e5765b8423dac3e5c657e6ee800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3b8a6753b04c6b831a6045e252638a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1675 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19275f0d789345f184a9092254b51d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 04:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.640362</td>\n",
       "      <td>0.252675</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.432678</td>\n",
       "      <td>0.224661</td>\n",
       "      <td>0.505747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.297076</td>\n",
       "      <td>0.286880</td>\n",
       "      <td>0.561514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731addf5393d40a3920b61b3b12ae4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 03:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.582546</td>\n",
       "      <td>0.266149</td>\n",
       "      <td>0.522013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.371616</td>\n",
       "      <td>0.232371</td>\n",
       "      <td>0.562319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.238770</td>\n",
       "      <td>0.245645</td>\n",
       "      <td>0.565598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8af668e1648468dbe018f4fb8031ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 04:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.699522</td>\n",
       "      <td>0.966720</td>\n",
       "      <td>0.170262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.703424</td>\n",
       "      <td>0.628728</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.685130</td>\n",
       "      <td>0.637917</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b904457d3ce94bddac98f068b4154a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 03:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.569855</td>\n",
       "      <td>0.269402</td>\n",
       "      <td>0.492754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.369737</td>\n",
       "      <td>0.238685</td>\n",
       "      <td>0.548476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.198963</td>\n",
       "      <td>0.246403</td>\n",
       "      <td>0.586826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7393900b447429c8c5ba1035829607f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e10f4555944291b7c7c970e8013a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1675 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53696636661e4413989e5a3708c12d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.706355</td>\n",
       "      <td>0.770580</td>\n",
       "      <td>0.179157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.571512</td>\n",
       "      <td>0.247420</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.462759</td>\n",
       "      <td>0.270995</td>\n",
       "      <td>0.559322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9273775f914743935e61273bc1cc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 03:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.547305</td>\n",
       "      <td>0.287314</td>\n",
       "      <td>0.502392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.374984</td>\n",
       "      <td>0.304882</td>\n",
       "      <td>0.539379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.247194</td>\n",
       "      <td>0.298766</td>\n",
       "      <td>0.537859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5599bb2bfaad4ffcbd03c20960f2b5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.608630</td>\n",
       "      <td>0.253170</td>\n",
       "      <td>0.485356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.453889</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.528986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.282931</td>\n",
       "      <td>0.331991</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abaa18b0a0eb4f7799d1d866d4defb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 03:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.539258</td>\n",
       "      <td>0.248009</td>\n",
       "      <td>0.525680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.391293</td>\n",
       "      <td>0.257332</td>\n",
       "      <td>0.579387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.217048</td>\n",
       "      <td>0.288746</td>\n",
       "      <td>0.536986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ba021712654b33bc7ac4eb5c4465a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883c724913894ef7a55eb7a3c976f844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1675 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fb3adf9c0444398fcf64f16986a21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.603286</td>\n",
       "      <td>0.284084</td>\n",
       "      <td>0.483951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.406834</td>\n",
       "      <td>0.231592</td>\n",
       "      <td>0.519380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.271454</td>\n",
       "      <td>0.307045</td>\n",
       "      <td>0.567073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c588144c374226847f727ebce08241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 03:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.564750</td>\n",
       "      <td>0.300509</td>\n",
       "      <td>0.531707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.348576</td>\n",
       "      <td>0.204554</td>\n",
       "      <td>0.595469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.221536</td>\n",
       "      <td>0.262597</td>\n",
       "      <td>0.597333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26f0b81cda5470c8ef2440c64d84cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.626001</td>\n",
       "      <td>0.323549</td>\n",
       "      <td>0.465374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.477659</td>\n",
       "      <td>0.249016</td>\n",
       "      <td>0.474308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.382433</td>\n",
       "      <td>0.314167</td>\n",
       "      <td>0.519481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2aac16929047c4957f15f6702b3377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 03:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.608071</td>\n",
       "      <td>0.365809</td>\n",
       "      <td>0.507317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.379896</td>\n",
       "      <td>0.211216</td>\n",
       "      <td>0.555957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.232405</td>\n",
       "      <td>0.262205</td>\n",
       "      <td>0.586895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8c91fe69da4358a2360f3c984e2075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc613d7b006b4075858c962f1daa9375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1675 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54909135dbb149c892d8710880b51d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.613819</td>\n",
       "      <td>0.229964</td>\n",
       "      <td>0.450593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.463934</td>\n",
       "      <td>0.250815</td>\n",
       "      <td>0.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.325602</td>\n",
       "      <td>0.313227</td>\n",
       "      <td>0.527607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d4d9e5b398476b88feb2ccba68bb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 03:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.550440</td>\n",
       "      <td>0.314677</td>\n",
       "      <td>0.503597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.352363</td>\n",
       "      <td>0.254729</td>\n",
       "      <td>0.554348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.222264</td>\n",
       "      <td>0.285404</td>\n",
       "      <td>0.559078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94c270ed4a2429091d40a33e9bd0b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.709617</td>\n",
       "      <td>0.545898</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.664039</td>\n",
       "      <td>0.347247</td>\n",
       "      <td>0.369620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.524604</td>\n",
       "      <td>0.367139</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae137081a464780b8a10df149bf9864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 03:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.552532</td>\n",
       "      <td>0.271543</td>\n",
       "      <td>0.482955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.363501</td>\n",
       "      <td>0.229156</td>\n",
       "      <td>0.516340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.213356</td>\n",
       "      <td>0.275991</td>\n",
       "      <td>0.515337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ad3cee14a54312b1ed33edcfe6cd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e5ab144f4544d29c7b70529fedf435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1675 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba128ada0a9447f812d628c1e04f811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.619364</td>\n",
       "      <td>0.292903</td>\n",
       "      <td>0.459459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.482848</td>\n",
       "      <td>0.227002</td>\n",
       "      <td>0.489451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.352226</td>\n",
       "      <td>0.297262</td>\n",
       "      <td>0.548673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff226f6a4f0495da59cd23f923b15b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 03:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.577434</td>\n",
       "      <td>0.243409</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.393531</td>\n",
       "      <td>0.247552</td>\n",
       "      <td>0.503067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.277445</td>\n",
       "      <td>0.317008</td>\n",
       "      <td>0.521519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc08296d30f49a4aa2a36a7b7579959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 03:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.636734</td>\n",
       "      <td>0.276559</td>\n",
       "      <td>0.456338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.517149</td>\n",
       "      <td>0.242057</td>\n",
       "      <td>0.379630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.413638</td>\n",
       "      <td>0.289767</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6bc792f32c4bd885b69497085cd040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 03:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.645945</td>\n",
       "      <td>0.361185</td>\n",
       "      <td>0.430155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.429827</td>\n",
       "      <td>0.238447</td>\n",
       "      <td>0.375546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.317793</td>\n",
       "      <td>0.305161</td>\n",
       "      <td>0.506394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Roberta': {'best_hps': (3e-05, 32), 'avg_f1': 0.5557551489307914}, 'Deberta': {'best_hps': (2e-05, 32), 'avg_f1': 0.5562773611889518}}\n"
     ]
    }
   ],
   "source": [
    "tuning_res = hyperparameter_search()\n",
    "print(tuning_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0978b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_res = {\n",
    "    \"Roberta\": {\n",
    "        \"best_hps\": (3e-5, 32),\n",
    "        \"avg_f1\": 0.5557551489307914\n",
    "    },\n",
    "    \"Deberta\": {\n",
    "        \"best_hps\": (2e-5, 32),\n",
    "        \"avg_f1\": 0.5562773611889518\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60232018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "FINAL_EPOCHS = 5 \n",
    "VAL_SIZE = 0.1\n",
    "WEIGHT_DECAY = 0.05\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# Split into train and val sets for monitoring best checkpoint\n",
    "trdf_train, trdf_val = train_test_split(trdf, test_size=VAL_SIZE, random_state=RANDOM_SEED, stratify=trdf['labels'])\n",
    "\n",
    "# Retrain best models on full training set\n",
    "def train_ensemble_models(tuning_results):\n",
    "    ensemble_models = {}\n",
    "\n",
    "    for name, checkpoint, tokenizer in architectures:\n",
    "        print(f\"Retraining {name} on full training set...\")\n",
    "\n",
    "        trds = Dataset.from_pandas(trdf_train)\n",
    "        valds = Dataset.from_pandas(trdf_val)\n",
    "\n",
    "        trds = trds.map(lambda batch: tokenize(tokenizer, batch), batched=True)\n",
    "        valds = valds.map(lambda batch: tokenize(tokenizer, batch), batched=True)\n",
    "        trds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "        valds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "        lr, bsz = tuning_results[name][\"best_hps\"]\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2,\n",
    "                                                                   hidden_dropout_prob=DROPOUT,\n",
    "                                                                   attention_probs_dropout_prob=DROPOUT).to(device)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'./final_results/{name}',\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=bsz,\n",
    "            per_device_eval_batch_size=bsz,\n",
    "            num_train_epochs=FINAL_EPOCHS,\n",
    "            logging_dir=f'./final_logs/{name}',\n",
    "            eval_strategy=\"epoch\",\n",
    "            logging_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=1,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "\n",
    "        trainer = BalancedTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=trds,\n",
    "            eval_dataset=valds,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        metrics = trainer.evaluate()\n",
    "\n",
    "        model = model.to(\"cpu\")\n",
    "\n",
    "        ensemble_models[name] = (model, tokenizer, metrics[\"eval_f1\"])\n",
    "    \n",
    "    tot_f1 = sum(m[2] for m in ensemble_models.values())\n",
    "    for name in ensemble_models:\n",
    "        model, tokenizer, f1 = ensemble_models[name]\n",
    "        ensemble_weight = f1 / tot_f1\n",
    "        ensemble_models[name] = (model, tokenizer, ensemble_weight)\n",
    "    \n",
    "    return ensemble_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea14b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Roberta on full training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5373f9e7209941b095e4c5f851f68f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7537 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45389faae98d414a82fa8585678ba74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49464aafd29b4b04bf672f3b26865650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: FacebookAI/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "`logging_dir` is deprecated and will be removed in v5.2. Please set `TENSORBOARD_LOGGING_DIR` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1180' max='1180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1180/1180 06:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.586013</td>\n",
       "      <td>0.556028</td>\n",
       "      <td>0.357955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.453254</td>\n",
       "      <td>0.455058</td>\n",
       "      <td>0.455696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.343820</td>\n",
       "      <td>0.570429</td>\n",
       "      <td>0.506912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.275210</td>\n",
       "      <td>0.547164</td>\n",
       "      <td>0.511416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.209140</td>\n",
       "      <td>0.681463</td>\n",
       "      <td>0.512077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93393f718674b55846697c3262d8945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc87016c7a94771aa7281b76f9b3275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2288f958aaa04d9fbc4bfe1b0bed4be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6a12a8aa494e0b9a3c26af674f0cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b46a1ffcc94368b1c8c834d0640b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['roberta.embeddings.LayerNorm.beta', 'roberta.embeddings.LayerNorm.gamma', 'roberta.encoder.layer.0.attention.output.LayerNorm.beta', 'roberta.encoder.layer.0.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.0.output.LayerNorm.beta', 'roberta.encoder.layer.0.output.LayerNorm.gamma', 'roberta.encoder.layer.1.attention.output.LayerNorm.beta', 'roberta.encoder.layer.1.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.1.output.LayerNorm.beta', 'roberta.encoder.layer.1.output.LayerNorm.gamma', 'roberta.encoder.layer.2.attention.output.LayerNorm.beta', 'roberta.encoder.layer.2.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.2.output.LayerNorm.beta', 'roberta.encoder.layer.2.output.LayerNorm.gamma', 'roberta.encoder.layer.3.attention.output.LayerNorm.beta', 'roberta.encoder.layer.3.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.3.output.LayerNorm.beta', 'roberta.encoder.layer.3.output.LayerNorm.gamma', 'roberta.encoder.layer.4.attention.output.LayerNorm.beta', 'roberta.encoder.layer.4.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.4.output.LayerNorm.beta', 'roberta.encoder.layer.4.output.LayerNorm.gamma', 'roberta.encoder.layer.5.attention.output.LayerNorm.beta', 'roberta.encoder.layer.5.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.5.output.LayerNorm.beta', 'roberta.encoder.layer.5.output.LayerNorm.gamma', 'roberta.encoder.layer.6.attention.output.LayerNorm.beta', 'roberta.encoder.layer.6.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.6.output.LayerNorm.beta', 'roberta.encoder.layer.6.output.LayerNorm.gamma', 'roberta.encoder.layer.7.attention.output.LayerNorm.beta', 'roberta.encoder.layer.7.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.7.output.LayerNorm.beta', 'roberta.encoder.layer.7.output.LayerNorm.gamma', 'roberta.encoder.layer.8.attention.output.LayerNorm.beta', 'roberta.encoder.layer.8.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.8.output.LayerNorm.beta', 'roberta.encoder.layer.8.output.LayerNorm.gamma', 'roberta.encoder.layer.9.attention.output.LayerNorm.beta', 'roberta.encoder.layer.9.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.9.output.LayerNorm.beta', 'roberta.encoder.layer.9.output.LayerNorm.gamma', 'roberta.encoder.layer.10.attention.output.LayerNorm.beta', 'roberta.encoder.layer.10.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.10.output.LayerNorm.beta', 'roberta.encoder.layer.10.output.LayerNorm.gamma', 'roberta.encoder.layer.11.attention.output.LayerNorm.beta', 'roberta.encoder.layer.11.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.11.output.LayerNorm.beta', 'roberta.encoder.layer.11.output.LayerNorm.gamma'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Deberta on full training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78babcbac4aa450fae1828bdc726921d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7537 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9c8758691e44c99d189ea60b253feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e182eecd734989bf0c8ee8aed3e288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                         | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "`logging_dir` is deprecated and will be removed in v5.2. Please set `TENSORBOARD_LOGGING_DIR` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1180' max='1180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1180/1180 07:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.665891</td>\n",
       "      <td>0.584425</td>\n",
       "      <td>0.309623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.462348</td>\n",
       "      <td>0.472078</td>\n",
       "      <td>0.495495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.345894</td>\n",
       "      <td>0.667231</td>\n",
       "      <td>0.486772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.274227</td>\n",
       "      <td>0.721025</td>\n",
       "      <td>0.525253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.203023</td>\n",
       "      <td>0.800883</td>\n",
       "      <td>0.489583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7f06d275bb4628850ab7a4130a2427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6dccdd6b0a4aa8b4de12dd73f2ff81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6fe69b85d44fc38b6a1c4e05250c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3711d0665241b986096c40c94df089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306cb9774df84aad91ff92f5af834749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['deberta.embeddings.LayerNorm.weight', 'deberta.embeddings.LayerNorm.bias', 'deberta.encoder.layer.0.attention.output.LayerNorm.weight', 'deberta.encoder.layer.0.attention.output.LayerNorm.bias', 'deberta.encoder.layer.0.output.LayerNorm.weight', 'deberta.encoder.layer.0.output.LayerNorm.bias', 'deberta.encoder.layer.1.attention.output.LayerNorm.weight', 'deberta.encoder.layer.1.attention.output.LayerNorm.bias', 'deberta.encoder.layer.1.output.LayerNorm.weight', 'deberta.encoder.layer.1.output.LayerNorm.bias', 'deberta.encoder.layer.2.attention.output.LayerNorm.weight', 'deberta.encoder.layer.2.attention.output.LayerNorm.bias', 'deberta.encoder.layer.2.output.LayerNorm.weight', 'deberta.encoder.layer.2.output.LayerNorm.bias', 'deberta.encoder.layer.3.attention.output.LayerNorm.weight', 'deberta.encoder.layer.3.attention.output.LayerNorm.bias', 'deberta.encoder.layer.3.output.LayerNorm.weight', 'deberta.encoder.layer.3.output.LayerNorm.bias', 'deberta.encoder.layer.4.attention.output.LayerNorm.weight', 'deberta.encoder.layer.4.attention.output.LayerNorm.bias', 'deberta.encoder.layer.4.output.LayerNorm.weight', 'deberta.encoder.layer.4.output.LayerNorm.bias', 'deberta.encoder.layer.5.attention.output.LayerNorm.weight', 'deberta.encoder.layer.5.attention.output.LayerNorm.bias', 'deberta.encoder.layer.5.output.LayerNorm.weight', 'deberta.encoder.layer.5.output.LayerNorm.bias', 'deberta.encoder.layer.6.attention.output.LayerNorm.weight', 'deberta.encoder.layer.6.attention.output.LayerNorm.bias', 'deberta.encoder.layer.6.output.LayerNorm.weight', 'deberta.encoder.layer.6.output.LayerNorm.bias', 'deberta.encoder.layer.7.attention.output.LayerNorm.weight', 'deberta.encoder.layer.7.attention.output.LayerNorm.bias', 'deberta.encoder.layer.7.output.LayerNorm.weight', 'deberta.encoder.layer.7.output.LayerNorm.bias', 'deberta.encoder.layer.8.attention.output.LayerNorm.weight', 'deberta.encoder.layer.8.attention.output.LayerNorm.bias', 'deberta.encoder.layer.8.output.LayerNorm.weight', 'deberta.encoder.layer.8.output.LayerNorm.bias', 'deberta.encoder.layer.9.attention.output.LayerNorm.weight', 'deberta.encoder.layer.9.attention.output.LayerNorm.bias', 'deberta.encoder.layer.9.output.LayerNorm.weight', 'deberta.encoder.layer.9.output.LayerNorm.bias', 'deberta.encoder.layer.10.attention.output.LayerNorm.weight', 'deberta.encoder.layer.10.attention.output.LayerNorm.bias', 'deberta.encoder.layer.10.output.LayerNorm.weight', 'deberta.encoder.layer.10.output.LayerNorm.bias', 'deberta.encoder.layer.11.attention.output.LayerNorm.weight', 'deberta.encoder.layer.11.attention.output.LayerNorm.bias', 'deberta.encoder.layer.11.output.LayerNorm.weight', 'deberta.encoder.layer.11.output.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['deberta.embeddings.LayerNorm.beta', 'deberta.embeddings.LayerNorm.gamma', 'deberta.encoder.layer.0.attention.output.LayerNorm.beta', 'deberta.encoder.layer.0.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.0.output.LayerNorm.beta', 'deberta.encoder.layer.0.output.LayerNorm.gamma', 'deberta.encoder.layer.1.attention.output.LayerNorm.beta', 'deberta.encoder.layer.1.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.1.output.LayerNorm.beta', 'deberta.encoder.layer.1.output.LayerNorm.gamma', 'deberta.encoder.layer.2.attention.output.LayerNorm.beta', 'deberta.encoder.layer.2.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.2.output.LayerNorm.beta', 'deberta.encoder.layer.2.output.LayerNorm.gamma', 'deberta.encoder.layer.3.attention.output.LayerNorm.beta', 'deberta.encoder.layer.3.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.3.output.LayerNorm.beta', 'deberta.encoder.layer.3.output.LayerNorm.gamma', 'deberta.encoder.layer.4.attention.output.LayerNorm.beta', 'deberta.encoder.layer.4.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.4.output.LayerNorm.beta', 'deberta.encoder.layer.4.output.LayerNorm.gamma', 'deberta.encoder.layer.5.attention.output.LayerNorm.beta', 'deberta.encoder.layer.5.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.5.output.LayerNorm.beta', 'deberta.encoder.layer.5.output.LayerNorm.gamma', 'deberta.encoder.layer.6.attention.output.LayerNorm.beta', 'deberta.encoder.layer.6.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.6.output.LayerNorm.beta', 'deberta.encoder.layer.6.output.LayerNorm.gamma', 'deberta.encoder.layer.7.attention.output.LayerNorm.beta', 'deberta.encoder.layer.7.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.7.output.LayerNorm.beta', 'deberta.encoder.layer.7.output.LayerNorm.gamma', 'deberta.encoder.layer.8.attention.output.LayerNorm.beta', 'deberta.encoder.layer.8.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.8.output.LayerNorm.beta', 'deberta.encoder.layer.8.output.LayerNorm.gamma', 'deberta.encoder.layer.9.attention.output.LayerNorm.beta', 'deberta.encoder.layer.9.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.9.output.LayerNorm.beta', 'deberta.encoder.layer.9.output.LayerNorm.gamma', 'deberta.encoder.layer.10.attention.output.LayerNorm.beta', 'deberta.encoder.layer.10.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.10.output.LayerNorm.beta', 'deberta.encoder.layer.10.output.LayerNorm.gamma', 'deberta.encoder.layer.11.attention.output.LayerNorm.beta', 'deberta.encoder.layer.11.attention.output.LayerNorm.gamma', 'deberta.encoder.layer.11.output.LayerNorm.beta', 'deberta.encoder.layer.11.output.LayerNorm.gamma'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Roberta': (RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      "), RobertaTokenizer(name_or_path='FacebookAI/roberta-base', vocab_size=50265, model_max_length=512, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
      "}\n",
      "), 0.49364944961896695), 'Deberta': (DebertaForSequenceClassification(\n",
      "  (deberta): DebertaModel(\n",
      "    (embeddings): DebertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
      "      (LayerNorm): DebertaLayerNorm()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (encoder): DebertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaLayer(\n",
      "          (attention): DebertaAttention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
      "              (pos_dropout): Dropout(p=0.2, inplace=False)\n",
      "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "            (output): DebertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): DebertaLayerNorm()\n",
      "              (dropout): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): DebertaLayerNorm()\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(1024, 768)\n",
      "    )\n",
      "  )\n",
      "  (pooler): ContextPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "), DebertaTokenizer(name_or_path='microsoft/deberta-base', vocab_size=50265, model_max_length=1000000000000000019884624838656, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50264: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 0.506350550381033)}\n"
     ]
    }
   ],
   "source": [
    "ensemble_models = train_ensemble_models(tuning_res)\n",
    "print(ensemble_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cbad7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move ensemble models to CPU\n",
    "for name in ensemble_models:\n",
    "    model, tokenizer, weight = ensemble_models[name]\n",
    "    ensemble_models[name] = (model.to(\"cpu\"), tokenizer, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c45705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BSZ=16\n",
    "\n",
    "def ensemble_predict(dataset, ensemble_models):\n",
    "    dataloader = DataLoader(dataset, batch_size=BSZ)\n",
    "    all_preds = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        texts = batch[\"text\"]\n",
    "        batch_probs = None\n",
    "\n",
    "        for model, tokenizer, weight in ensemble_models.values():\n",
    "            # Tokenize this batch for this model\n",
    "            encodings = tokenizer(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            input_ids = encodings[\"input_ids\"].to(device)\n",
    "            attention_mask = encodings[\"attention_mask\"].to(device)\n",
    "\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                probs = F.softmax(outputs.logits, dim=-1) * weight\n",
    "\n",
    "            # Accumulate weighted probabilities\n",
    "            if batch_probs is None:\n",
    "                batch_probs = probs\n",
    "            else:\n",
    "                batch_probs += probs\n",
    "\n",
    "            # Move model back to CPU to free GPU memory\n",
    "            model.to(\"cpu\")\n",
    "\n",
    "        # Final predictions for this batch\n",
    "        preds = torch.argmax(batch_probs, dim=-1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81d8dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to save predictions to an output file\n",
    "def labels2file(p, outf_path):\n",
    "    with open(outf_path, \"w\") as f:\n",
    "        for pred in p:\n",
    "            f.write(f\"{pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "valds = Dataset.from_pandas(trdf_val)\n",
    "\n",
    "# Generate training dev set predictions using ensemble\n",
    "preds = ensemble_predict(valds, ensemble_models)\n",
    "labels2file(preds, os.path.join('res/', 'tval.txt'))\n",
    "\n",
    "# Save reference labels for training dev set\n",
    "labels2file(trdf_val['labels'].tolist(), os.path.join('ref/', 'tval.txt'))\n",
    "\n",
    "!python3 evaluation.py . . tval.txt\n",
    "!cat scores.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d791f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "valds = Dataset.from_pandas(valdf)\n",
    "\n",
    "# Generate dev set predictions using ensemble\n",
    "preds = ensemble_predict(valds, ensemble_models)\n",
    "labels2file(preds, os.path.join('res/', 'dev.txt'))\n",
    "\n",
    "# Save reference labels for dev set\n",
    "labels2file(valdf['labels'].tolist(), os.path.join('ref/', 'dev.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0bfe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 evaluation.py . . dev.txt\n",
    "!cat scores.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4dc8ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  par_id                                               text  labels\n",
      "0    t_4  Members of the church , which is part of Ken C...       1\n",
      "1    t_5  \"To ensure that \"\" Priority Agriculture Progra...       1\n",
      "2    t_6  The deportees stepped off their flight from El...       1\n",
      "3    t_7  PIMS staffer who raped disabled girl at ICU wa...       1\n",
      "4    t_9  I conclude , Yes , the general FEELING generat...       1\n"
     ]
    }
   ],
   "source": [
    "dpm.load_test()\n",
    "\n",
    "cols = ['par_id', 'text', 'label']\n",
    "testdf = dpm.test_set_df.copy()\n",
    "testdf = testdf[cols]\n",
    "testdf.rename(columns={\"label\": \"labels\"}, inplace=True)\n",
    "print(testdf.head())\n",
    "\n",
    "testds = Dataset.from_pandas(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673cc707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test set predictions\n",
    "preds = ensemble_predict(testds, ensemble_models)\n",
    "labels2file(preds, os.path.join('res/', 'test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9350e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
